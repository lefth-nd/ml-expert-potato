{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lefth-nd/ml-expert-potato/blob/main/COMP2712_Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVDcdZVVIswy"
      },
      "source": [
        "# COMP2712_8715 Assignment01: Classifying Images Using MLP and SVM Algorithms \n",
        "This assignment is to explore the concepts covered in the topic so far with respect to training and evaluating classifiers such as the Multi-layer Perceptron (MLP) and Support Vector Machines (SVM).  \n",
        "\n",
        "This starter Notebook can be used as a starter for this assignment.  It contains examples for reading and manipulating the dataset and training a standard MLP.  You should use this notebook in conjunction with the Assignment Specification that can be found on the topic FLO page: [COMP2712_8715](https://flo.flinders.edu.au/course/view.php?id=71089#section-17)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC0Dh4dwQ0Iq"
      },
      "source": [
        "## Accessing Data from Google Drive\n",
        "The dataset for this assignment is the CIFAR-10 dataset that can be found here:\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html \n",
        "\n",
        "The CIFAR-10 and CIFAR-100 are well studied, yet challenging image recognition dataset. The CIFAR-10 has up to 10 classes to classify and contains 60,000 32x32 images. You should read the description of the dataset and download the dataset for Python, that is\n",
        "\n",
        "CIFAR-10 python version: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz \n",
        "\n",
        "Once downloaded you need to then extract and upload the `cifar-10-batches-py` directory your Google Drive so that you can access it from within your Google Colab.\n",
        "\n",
        "You can mount the Google Drive from the menu on the left or uncomment use the code below mount the drive.  See here for documentation on file access in Colab:\n",
        "\n",
        "[External data: Local Files, Drive, Sheets, and Cloud Storage](https://colab.research.google.com/notebooks/io.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEHloSne7FtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24582757-1603-47e5-e2bc-565b986ca8a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6-0Jo9BRmNy"
      },
      "source": [
        "# uncomment the below to mount the google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s7DsaxDI2im"
      },
      "source": [
        "## Functions to work with CIFAR\n",
        "\n",
        "The functions below help with access to the CIFAR-10 data the you have downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUSVhQr5KDv1"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_CIFAR_batch(filename, flatten=True, categorical=True):\n",
        "    \"\"\" load single batch of cifar \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = pickle.load(f, encoding='bytes')\n",
        "        X = datadict[b'data']        \n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
        "        if (flatten):\n",
        "          X = X.reshape(10000, 3072)\n",
        "        X = X.astype('float32')\n",
        "        X /= 255\n",
        "\n",
        "        y = datadict[b'labels']\n",
        "        y = np.array(y)\n",
        "        if (categorical):\n",
        "          y = pd.get_dummies(y).values\n",
        "\n",
        "        return X, y\n",
        "\n",
        "def load_CIFAR_meta(filename):\n",
        "  with open(filename,'rb') as f:\n",
        "    metadict = pickle.load(f, encoding='bytes')\n",
        "\n",
        "    class_labels = [ val.decode() for val in metadict.get(b'label_names') ]\n",
        "    return class_labels\n",
        "\n",
        "def get_image(X, index, nchans=3, size=32):\n",
        "  xi = X[index,:]\n",
        "  img = xi.reshape(32, 32, 3)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axqceDnNJVss"
      },
      "source": [
        "## Load the CIFAR data\n",
        "\n",
        "The CIFAR data has 5 batches of data and 1 test data set. Each batch is labelled\n",
        "- `data_batch_1`\n",
        "- `data_batch_2`\n",
        "- `data_batch_3`\n",
        "- `data_batch_4`\n",
        "- `data_batch_5`\n",
        "\n",
        "and a test set labelled\n",
        "- `test_batch`\n",
        "\n",
        "each batch has 10,000 images, so 50,000 training and 10,000 test images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2X0_Oz_SkHi"
      },
      "source": [
        "Below is example of loading the first batch of training data labelled as `data_batch_1`.  You will need to update the path to match where you have stored your cifar-10 data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tuyy6hKKFhx"
      },
      "source": [
        "X, y = load_CIFAR_batch('/content/drive/My Drive/COMP2712/data/cifar-10-batches-py/data_batch_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMUnAt_YKPjp"
      },
      "source": [
        "The number of instances/examples for all the different classes.  There are 10 different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-uX5AZmQG9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a1610e-f3b7-4d7b-e371-d0ddc200186f"
      },
      "source": [
        "[np.sum(np.argmax(y, axis=1) == i) for i in range(0,10)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1005, 974, 1032, 1016, 999, 937, 1030, 1001, 1025, 981]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_HIgldKlsd"
      },
      "source": [
        "The labels for the classes are stored in the `batches.meta` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HKjGtxStmcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a9e7cc-8331-4431-afb7-f97b553490c6"
      },
      "source": [
        "class_labels = load_CIFAR_meta('/content/drive/MyDrive/COMP2712/data/cifar-10-batches-py/batches.meta')\n",
        "print(class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYNjse_BTLnx"
      },
      "source": [
        "Let's look at some random cat images, because cats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoI3IO7KTjMg"
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_random(X, y, class_labels, what_target='cat'):\n",
        "\n",
        "  what_label = class_labels.index(what_target)\n",
        "  what_labeli = np.where(np.argmax(y,axis=1) == what_label)[0]\n",
        "  random_what_index = what_labeli[random.choice(range(len(what_labeli)))]\n",
        "\n",
        "  plt.imshow(get_image(X,random_what_index))\n",
        "  plt.title('{}, index: {}'.format(what_target,random_what_index))\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AiPII3JUm3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "e94edc66-696f-419f-f8ba-a40f47e593ed"
      },
      "source": [
        "plot_random(X, y, class_labels, what_target='cat')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v557teUwb50v"
      },
      "source": [
        "## Set up and train MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZk2OEWpb83j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dfed0f8-c849-4d16-a867-67904a27dc16"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# split into train and test\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,stratify=y)\n",
        "\n",
        "\n",
        "hidden_layer_sizes = 100\n",
        "activation_fcn = 'relu' # {identity, logistic, tanh, relu}, default='relu'\n",
        "max_iter=200\n",
        "\n",
        "# model initialization\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(hidden_layer_sizes), activation=activation_fcn, \n",
        "                    max_iter=max_iter, #try change hidden layer, or max_iter\n",
        "                    solver='adam', verbose=1,   #try verbose=0 to train with out logging\n",
        "                    random_state=None,\n",
        "                    tol=0.00001,n_iter_no_change=20) # decreasing tol and increasing no_change will train the network 'harder' and usually longer\n",
        "\n",
        "mlp.fit(Xtrain, ytrain) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.40912265\n",
            "Iteration 2, loss = 2.98938861\n",
            "Iteration 3, loss = 2.92243669\n",
            "Iteration 4, loss = 2.88780166\n",
            "Iteration 5, loss = 2.82222558\n",
            "Iteration 6, loss = 2.77691084\n",
            "Iteration 7, loss = 2.76015759\n",
            "Iteration 8, loss = 2.72223573\n",
            "Iteration 9, loss = 2.68073712\n",
            "Iteration 10, loss = 2.65805174\n",
            "Iteration 11, loss = 2.63879238\n",
            "Iteration 12, loss = 2.60015923\n",
            "Iteration 13, loss = 2.60601310\n",
            "Iteration 14, loss = 2.58066220\n",
            "Iteration 15, loss = 2.58104181\n",
            "Iteration 16, loss = 2.55165369\n",
            "Iteration 17, loss = 2.51741668\n",
            "Iteration 18, loss = 2.50525718\n",
            "Iteration 19, loss = 2.48408505\n",
            "Iteration 20, loss = 2.47580677\n",
            "Iteration 21, loss = 2.47686140\n",
            "Iteration 22, loss = 2.45320221\n",
            "Iteration 23, loss = 2.43439836\n",
            "Iteration 24, loss = 2.40442063\n",
            "Iteration 25, loss = 2.41293413\n",
            "Iteration 26, loss = 2.39066661\n",
            "Iteration 27, loss = 2.39603738\n",
            "Iteration 28, loss = 2.39852624\n",
            "Iteration 29, loss = 2.36776553\n",
            "Iteration 30, loss = 2.37361233\n",
            "Iteration 31, loss = 2.35850609\n",
            "Iteration 32, loss = 2.33008832\n",
            "Iteration 33, loss = 2.34588716\n",
            "Iteration 34, loss = 2.32251883\n",
            "Iteration 35, loss = 2.32515315\n",
            "Iteration 36, loss = 2.30891879\n",
            "Iteration 37, loss = 2.31041134\n",
            "Iteration 38, loss = 2.28474117\n",
            "Iteration 39, loss = 2.28170164\n",
            "Iteration 40, loss = 2.24833314\n",
            "Iteration 41, loss = 2.24483553\n",
            "Iteration 42, loss = 2.24146937\n",
            "Iteration 43, loss = 2.25089670\n",
            "Iteration 44, loss = 2.21456446\n",
            "Iteration 45, loss = 2.22388507\n",
            "Iteration 46, loss = 2.20125776\n",
            "Iteration 47, loss = 2.20944359\n",
            "Iteration 48, loss = 2.19905428\n",
            "Iteration 49, loss = 2.18346148\n",
            "Iteration 50, loss = 2.17844746\n",
            "Iteration 51, loss = 2.15353200\n",
            "Iteration 52, loss = 2.14630839\n",
            "Iteration 53, loss = 2.15450851\n",
            "Iteration 54, loss = 2.16411285\n",
            "Iteration 55, loss = 2.14048424\n",
            "Iteration 56, loss = 2.14542319\n",
            "Iteration 57, loss = 2.11987519\n",
            "Iteration 58, loss = 2.12325718\n",
            "Iteration 59, loss = 2.12255910\n",
            "Iteration 60, loss = 2.09729125\n",
            "Iteration 61, loss = 2.10040628\n",
            "Iteration 62, loss = 2.08486173\n",
            "Iteration 63, loss = 2.08970314\n",
            "Iteration 64, loss = 2.09597554\n",
            "Iteration 65, loss = 2.09513960\n",
            "Iteration 66, loss = 2.05600298\n",
            "Iteration 67, loss = 2.05541761\n",
            "Iteration 68, loss = 2.02660158\n",
            "Iteration 69, loss = 2.03129130\n",
            "Iteration 70, loss = 2.04801869\n",
            "Iteration 71, loss = 2.01497015\n",
            "Iteration 72, loss = 2.01475780\n",
            "Iteration 73, loss = 1.99385168\n",
            "Iteration 74, loss = 1.99692304\n",
            "Iteration 75, loss = 1.98751655\n",
            "Iteration 76, loss = 1.97803351\n",
            "Iteration 77, loss = 1.99419983\n",
            "Iteration 78, loss = 1.96353799\n",
            "Iteration 79, loss = 1.95835200\n",
            "Iteration 80, loss = 1.96517545\n",
            "Iteration 81, loss = 1.95765774\n",
            "Iteration 82, loss = 1.94179415\n",
            "Iteration 83, loss = 1.94665767\n",
            "Iteration 84, loss = 1.94733850\n",
            "Iteration 85, loss = 1.93531494\n",
            "Iteration 86, loss = 1.95232989\n",
            "Iteration 87, loss = 1.92235804\n",
            "Iteration 88, loss = 1.92388221\n",
            "Iteration 89, loss = 1.91980358\n",
            "Iteration 90, loss = 1.88167871\n",
            "Iteration 91, loss = 1.88486917\n",
            "Iteration 92, loss = 1.90501185\n",
            "Iteration 93, loss = 1.89101217\n",
            "Iteration 94, loss = 1.89844700\n",
            "Iteration 95, loss = 1.87784129\n",
            "Iteration 96, loss = 1.87564519\n",
            "Iteration 97, loss = 1.86181961\n",
            "Iteration 98, loss = 1.85287289\n",
            "Iteration 99, loss = 1.84700717\n",
            "Iteration 100, loss = 1.86552641\n",
            "Iteration 101, loss = 1.84410166\n",
            "Iteration 102, loss = 1.87082641\n",
            "Iteration 103, loss = 1.81851927\n",
            "Iteration 104, loss = 1.82840445\n",
            "Iteration 105, loss = 1.83117464\n",
            "Iteration 106, loss = 1.82264828\n",
            "Iteration 107, loss = 1.80514205\n",
            "Iteration 108, loss = 1.83084592\n",
            "Iteration 109, loss = 1.80391633\n",
            "Iteration 110, loss = 1.80926264\n",
            "Iteration 111, loss = 1.77636381\n",
            "Iteration 112, loss = 1.77777917\n",
            "Iteration 113, loss = 1.78381798\n",
            "Iteration 114, loss = 1.76043889\n",
            "Iteration 115, loss = 1.75894119\n",
            "Iteration 116, loss = 1.77959076\n",
            "Iteration 117, loss = 1.76328524\n",
            "Iteration 118, loss = 1.76444877\n",
            "Iteration 119, loss = 1.73336199\n",
            "Iteration 120, loss = 1.74753992\n",
            "Iteration 121, loss = 1.72447294\n",
            "Iteration 122, loss = 1.75693891\n",
            "Iteration 123, loss = 1.72667419\n",
            "Iteration 124, loss = 1.74711579\n",
            "Iteration 125, loss = 1.70216891\n",
            "Iteration 126, loss = 1.69286095\n",
            "Iteration 127, loss = 1.73598759\n",
            "Iteration 128, loss = 1.71380810\n",
            "Iteration 129, loss = 1.72728044\n",
            "Iteration 130, loss = 1.69332031\n",
            "Iteration 131, loss = 1.68147600\n",
            "Iteration 132, loss = 1.68356996\n",
            "Iteration 133, loss = 1.67562328\n",
            "Iteration 134, loss = 1.66111567\n",
            "Iteration 135, loss = 1.66689178\n",
            "Iteration 136, loss = 1.70404575\n",
            "Iteration 137, loss = 1.67336134\n",
            "Iteration 138, loss = 1.65899266\n",
            "Iteration 139, loss = 1.62455064\n",
            "Iteration 140, loss = 1.66898978\n",
            "Iteration 141, loss = 1.67903502\n",
            "Iteration 142, loss = 1.64462413\n",
            "Iteration 143, loss = 1.61999712\n",
            "Iteration 144, loss = 1.62659447\n",
            "Iteration 145, loss = 1.61225775\n",
            "Iteration 146, loss = 1.65021189\n",
            "Iteration 147, loss = 1.61131458\n",
            "Iteration 148, loss = 1.58915118\n",
            "Iteration 149, loss = 1.61565051\n",
            "Iteration 150, loss = 1.60819912\n",
            "Iteration 151, loss = 1.58396999\n",
            "Iteration 152, loss = 1.59355269\n",
            "Iteration 153, loss = 1.61178480\n",
            "Iteration 154, loss = 1.60044952\n",
            "Iteration 155, loss = 1.60107228\n",
            "Iteration 156, loss = 1.55993301\n",
            "Iteration 157, loss = 1.56925766\n",
            "Iteration 158, loss = 1.56079773\n",
            "Iteration 159, loss = 1.57819985\n",
            "Iteration 160, loss = 1.54891670\n",
            "Iteration 161, loss = 1.55111276\n",
            "Iteration 162, loss = 1.53894365\n",
            "Iteration 163, loss = 1.55564823\n",
            "Iteration 164, loss = 1.52165115\n",
            "Iteration 165, loss = 1.52723732\n",
            "Iteration 166, loss = 1.52443404\n",
            "Iteration 167, loss = 1.50363615\n",
            "Iteration 168, loss = 1.51684734\n",
            "Iteration 169, loss = 1.51586792\n",
            "Iteration 170, loss = 1.50449579\n",
            "Iteration 171, loss = 1.52402338\n",
            "Iteration 172, loss = 1.53516430\n",
            "Iteration 173, loss = 1.51201351\n",
            "Iteration 174, loss = 1.51903441\n",
            "Iteration 175, loss = 1.52239727\n",
            "Iteration 176, loss = 1.54549726\n",
            "Iteration 177, loss = 1.47737070\n",
            "Iteration 178, loss = 1.45209978\n",
            "Iteration 179, loss = 1.48493715\n",
            "Iteration 180, loss = 1.46804210\n",
            "Iteration 181, loss = 1.45582152\n",
            "Iteration 182, loss = 1.48600627\n",
            "Iteration 183, loss = 1.44864014\n",
            "Iteration 184, loss = 1.44802901\n",
            "Iteration 185, loss = 1.47507815\n",
            "Iteration 186, loss = 1.44890792\n",
            "Iteration 187, loss = 1.46314633\n",
            "Iteration 188, loss = 1.43571089\n",
            "Iteration 189, loss = 1.42009530\n",
            "Iteration 190, loss = 1.42816977\n",
            "Iteration 191, loss = 1.42453164\n",
            "Iteration 192, loss = 1.42849011\n",
            "Iteration 193, loss = 1.43425862\n",
            "Iteration 194, loss = 1.40970325\n",
            "Iteration 195, loss = 1.40435920\n",
            "Iteration 196, loss = 1.39969570\n",
            "Iteration 197, loss = 1.37458137\n",
            "Iteration 198, loss = 1.39701413\n",
            "Iteration 199, loss = 1.42431486\n",
            "Iteration 200, loss = 1.38768636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=100, learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=20, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=1, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZI49FpD0vlU"
      },
      "source": [
        "## Evaluate the performance of the classfier\n",
        "\n",
        "\n",
        "\n",
        "Need to remember to use `predict_proba` for the multiclass classification and apply a softmax/argmax to the output (i.e. choose the largest probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekJURHGV04kT"
      },
      "source": [
        "### Test it out on a random image from the training data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpnHovVgZWmt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "ebd0f9c0-c1dd-4f79-a6b1-a8f425883214"
      },
      "source": [
        "idx = random.choice(range(Xtrain.shape[0]))\n",
        "print('Image Index: ',idx)\n",
        "\n",
        "# make sure you use predict_proba for the multi-class classification\n",
        "ypreda = mlp.predict_proba(Xtrain)\n",
        "print('Probability of class: {}'.format((ypreda[idx,:]*100).astype(int)))\n",
        "ypred = np.argmax(ypreda[idx,:], axis=0)\n",
        "ytrue = np.argmax(ytrain[idx,:], axis=0)\n",
        "print('real class     : ',ytrue,'=>',class_labels[ytrue])\n",
        "print('predicted class: ',ypred,'=>',class_labels[ypred])\n",
        "\n",
        "\n",
        "plt.imshow(get_image(Xtrain,idx))\n",
        "plt.title('Index: {}, R {}, P: {}'.format(idx,class_labels[ytrue],class_labels[ypred]))\n",
        "plt.axis('off');\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Index:  5975\n",
            "Probability of class: [ 0  0 13  3 38 16  5  2  0  5]\n",
            "real class     :  2 => bird\n",
            "predicted class:  4 => deer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGIomVQ50cKF"
      },
      "source": [
        "### Evaluate the performance on the whole training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlZcs-1Kem3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667999fe-9259-427c-afe8-fcb8dbb4134c"
      },
      "source": [
        "ytrue = np.argmax(ytrain, axis=1)\n",
        "ypreda = mlp.predict_proba(Xtrain)\n",
        "ypred = np.argmax(ypreda, axis=1)\n",
        "report = classification_report(ytrue, ypred)\n",
        "print(report)\n",
        "\n",
        "mat = confusion_matrix(ytrue,ypred)\n",
        "print(mat)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.85      0.78       754\n",
            "           1       0.94      0.86      0.90       730\n",
            "           2       0.63      0.73      0.68       774\n",
            "           3       0.66      0.73      0.70       762\n",
            "           4       0.81      0.57      0.67       749\n",
            "           5       0.82      0.58      0.68       703\n",
            "           6       0.68      0.88      0.77       772\n",
            "           7       0.88      0.77      0.82       751\n",
            "           8       0.86      0.76      0.81       769\n",
            "           9       0.80      0.92      0.86       736\n",
            "\n",
            "    accuracy                           0.77      7500\n",
            "   macro avg       0.78      0.77      0.77      7500\n",
            "weighted avg       0.78      0.77      0.77      7500\n",
            "\n",
            "[[641   5  33  10   4   1  15   5  25  15]\n",
            " [ 19 626   3   5   5   2   9   4  13  44]\n",
            " [ 43   2 565  35  29  15  50  16   8  11]\n",
            " [ 13   2  42 560   8  29  73   9   7  19]\n",
            " [ 36   6 103  25 426  21  83  25  14  10]\n",
            " [ 12   1  61 130  14 409  49  13   5   9]\n",
            " [  6   3  24  28  14   6 683   2   5   1]\n",
            " [ 27   3  34  33  18   8  20 579   6  23]\n",
            " [ 90   7  18  15   5   3  11   0 586  34]\n",
            " [  6  10  10   7   2   5   7   5   9 675]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miVFoBeX1F_k"
      },
      "source": [
        "### Evaluate on the performance on testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6-UC3c9iIry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effd1994-a714-4a10-a58d-0f6094a4bbc5"
      },
      "source": [
        "ytrue = np.argmax(ytest, axis=1)\n",
        "ypreda = mlp.predict_proba(Xtest)\n",
        "ypred = np.argmax(ypreda, axis=1)\n",
        "report = classification_report(ytrue, ypred)\n",
        "print(report)\n",
        "\n",
        "mat = confusion_matrix(ytrue,ypred)\n",
        "print(mat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.55      0.50       251\n",
            "           1       0.49      0.38      0.43       244\n",
            "           2       0.30      0.37      0.33       258\n",
            "           3       0.24      0.28      0.26       254\n",
            "           4       0.40      0.27      0.32       250\n",
            "           5       0.32      0.17      0.22       234\n",
            "           6       0.34      0.51      0.41       258\n",
            "           7       0.55      0.46      0.50       250\n",
            "           8       0.53      0.45      0.48       256\n",
            "           9       0.42      0.52      0.46       245\n",
            "\n",
            "    accuracy                           0.40      2500\n",
            "   macro avg       0.41      0.40      0.39      2500\n",
            "weighted avg       0.41      0.40      0.39      2500\n",
            "\n",
            "[[137   9  27   8   4   5  13   7  26  15]\n",
            " [ 18  92   9  21   8   4  15   9  19  49]\n",
            " [ 22   9  96  26  23  16  30  19  11   6]\n",
            " [  9   8  28  72  18  22  49  12  10  26]\n",
            " [ 17   7  49  19  68   8  56  13   6   7]\n",
            " [  4   7  33  67   8  39  44  17   4  11]\n",
            " [  1   6  29  36  20  12 132   5   5  12]\n",
            " [ 14   4  29  20  16   9  21 116   4  17]\n",
            " [ 54  11  10  17   4   2   7   3 114  34]\n",
            " [ 20  33   7  12   1   4  16   9  16 127]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1glCdfrjwyXD"
      },
      "source": [
        "## Try just classifying two classes\n",
        "\n",
        "Classfiying all 10 classes can be challenging.  Let's try a simplier task of classfiy just two distinct classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZVydf5gA5Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47832e00-0d26-46f8-edf5-6b3df40ddc6d"
      },
      "source": [
        "print(class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPoE0JeNm_Lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cdf2301-ea13-4e60-9358-251df55a09bc"
      },
      "source": [
        "one_target = 'frog'\n",
        "one_label = class_labels.index(one_target)\n",
        "print('Choose ', class_labels[one_label],'as the first class')\n",
        "one_labeli = np.where(np.argmax(y,axis=1) == one_label)[0]\n",
        "print('There are {} instances for the label {}'.format(len(one_labeli),one_target))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose  frog as the first class\n",
            "There are 1030 instances for the label frog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXYZS8udyfNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1b75c9-8e03-45d3-c501-9b128ef29fac"
      },
      "source": [
        "two_target = 'horse'\n",
        "two_label = class_labels.index(two_target)\n",
        "print('Choose ', class_labels[two_label],'as the first class')\n",
        "two_labeli = np.where(np.argmax(y,axis=1) == two_label)[0]\n",
        "print('There are {} instances for the label {}'.format(len(two_labeli),two_target))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose  horse as the first class\n",
            "There are 1001 instances for the label horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWyPyJg9zGN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0859df33-c131-4154-ffca-961faa431e48"
      },
      "source": [
        "alli = np.hstack((one_labeli,two_labeli))\n",
        "Xsubset = X[alli,:]\n",
        "\n",
        "# construct a new y with just the one and two label\n",
        "ysubset = y[alli,:]\n",
        "yclass = np.argmax(ysubset,axis=1)\n",
        "yclass[yclass == one_label] = 1\n",
        "yclass[yclass == two_label] = 2\n",
        "print(yclass)\n",
        "ysubset = pd.get_dummies(yclass).values\n",
        "print(ysubset)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 2 2 2]\n",
            "[[1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " ...\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6bJyAfh-5r4"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYOVtRse02Jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b57bd5-9ee9-4d71-88f6-fe1b3b375f91"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# split into train and test\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(Xsubset, ysubset,stratify=ysubset)\n",
        "\n",
        "\n",
        "hidden_layer_sizes = 100\n",
        "activation_fcn = 'relu' # {identity, logistic, tanh, relu}, default='relu'\n",
        "max_iter=200\n",
        "\n",
        "# model initialization\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(hidden_layer_sizes), activation=activation_fcn, \n",
        "                    max_iter=max_iter, #try change hidden layer, or max_iter\n",
        "                    solver='adam', verbose=1,   #try verbose=0 to train with out logging\n",
        "                    random_state=None,\n",
        "                    tol=0.00001,n_iter_no_change=20) # decreasing tol and increasing no_change will train the network 'harder' and usually longer\n",
        "\n",
        "mlp.fit(Xtrain, ytrain) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.65555815\n",
            "Iteration 2, loss = 1.21017618\n",
            "Iteration 3, loss = 1.07462821\n",
            "Iteration 4, loss = 1.00470575\n",
            "Iteration 5, loss = 0.96421779\n",
            "Iteration 6, loss = 0.89920687\n",
            "Iteration 7, loss = 0.85409230\n",
            "Iteration 8, loss = 0.83058493\n",
            "Iteration 9, loss = 0.79271284\n",
            "Iteration 10, loss = 0.76675287\n",
            "Iteration 11, loss = 0.74971061\n",
            "Iteration 12, loss = 0.72437875\n",
            "Iteration 13, loss = 0.72562605\n",
            "Iteration 14, loss = 0.72380068\n",
            "Iteration 15, loss = 0.68174116\n",
            "Iteration 16, loss = 0.70297869\n",
            "Iteration 17, loss = 0.69444955\n",
            "Iteration 18, loss = 0.69538693\n",
            "Iteration 19, loss = 0.68285892\n",
            "Iteration 20, loss = 0.68696362\n",
            "Iteration 21, loss = 0.70892410\n",
            "Iteration 22, loss = 0.64820796\n",
            "Iteration 23, loss = 0.60191009\n",
            "Iteration 24, loss = 0.59648608\n",
            "Iteration 25, loss = 0.59020164\n",
            "Iteration 26, loss = 0.60127480\n",
            "Iteration 27, loss = 0.56597460\n",
            "Iteration 28, loss = 0.55565753\n",
            "Iteration 29, loss = 0.56100811\n",
            "Iteration 30, loss = 0.53529119\n",
            "Iteration 31, loss = 0.53745594\n",
            "Iteration 32, loss = 0.57921729\n",
            "Iteration 33, loss = 0.53816322\n",
            "Iteration 34, loss = 0.51952281\n",
            "Iteration 35, loss = 0.53443754\n",
            "Iteration 36, loss = 0.53824231\n",
            "Iteration 37, loss = 0.49079082\n",
            "Iteration 38, loss = 0.48987153\n",
            "Iteration 39, loss = 0.48767482\n",
            "Iteration 40, loss = 0.46864134\n",
            "Iteration 41, loss = 0.48488667\n",
            "Iteration 42, loss = 0.48698809\n",
            "Iteration 43, loss = 0.46254084\n",
            "Iteration 44, loss = 0.45412016\n",
            "Iteration 45, loss = 0.46858077\n",
            "Iteration 46, loss = 0.45992132\n",
            "Iteration 47, loss = 0.45538578\n",
            "Iteration 48, loss = 0.45661187\n",
            "Iteration 49, loss = 0.45678169\n",
            "Iteration 50, loss = 0.43311675\n",
            "Iteration 51, loss = 0.41001806\n",
            "Iteration 52, loss = 0.42049704\n",
            "Iteration 53, loss = 0.43576421\n",
            "Iteration 54, loss = 0.41332875\n",
            "Iteration 55, loss = 0.40894154\n",
            "Iteration 56, loss = 0.38520622\n",
            "Iteration 57, loss = 0.45402918\n",
            "Iteration 58, loss = 0.46864045\n",
            "Iteration 59, loss = 0.42389848\n",
            "Iteration 60, loss = 0.39242111\n",
            "Iteration 61, loss = 0.38325469\n",
            "Iteration 62, loss = 0.37531826\n",
            "Iteration 63, loss = 0.35641740\n",
            "Iteration 64, loss = 0.35990623\n",
            "Iteration 65, loss = 0.35018224\n",
            "Iteration 66, loss = 0.34412426\n",
            "Iteration 67, loss = 0.34611983\n",
            "Iteration 68, loss = 0.33960436\n",
            "Iteration 69, loss = 0.35584526\n",
            "Iteration 70, loss = 0.33069938\n",
            "Iteration 71, loss = 0.32592349\n",
            "Iteration 72, loss = 0.31716541\n",
            "Iteration 73, loss = 0.31674139\n",
            "Iteration 74, loss = 0.33564553\n",
            "Iteration 75, loss = 0.30216893\n",
            "Iteration 76, loss = 0.30097077\n",
            "Iteration 77, loss = 0.30267772\n",
            "Iteration 78, loss = 0.28814188\n",
            "Iteration 79, loss = 0.28817151\n",
            "Iteration 80, loss = 0.28326029\n",
            "Iteration 81, loss = 0.28230634\n",
            "Iteration 82, loss = 0.28153295\n",
            "Iteration 83, loss = 0.28158793\n",
            "Iteration 84, loss = 0.29822629\n",
            "Iteration 85, loss = 0.27991015\n",
            "Iteration 86, loss = 0.27450885\n",
            "Iteration 87, loss = 0.27340570\n",
            "Iteration 88, loss = 0.27367786\n",
            "Iteration 89, loss = 0.25158596\n",
            "Iteration 90, loss = 0.24611841\n",
            "Iteration 91, loss = 0.25473522\n",
            "Iteration 92, loss = 0.27116725\n",
            "Iteration 93, loss = 0.30521725\n",
            "Iteration 94, loss = 0.24406858\n",
            "Iteration 95, loss = 0.23336962\n",
            "Iteration 96, loss = 0.23229430\n",
            "Iteration 97, loss = 0.23196606\n",
            "Iteration 98, loss = 0.22808936\n",
            "Iteration 99, loss = 0.22475193\n",
            "Iteration 100, loss = 0.22145355\n",
            "Iteration 101, loss = 0.23821902\n",
            "Iteration 102, loss = 0.24990864\n",
            "Iteration 103, loss = 0.24096927\n",
            "Iteration 104, loss = 0.21272480\n",
            "Iteration 105, loss = 0.23417819\n",
            "Iteration 106, loss = 0.21794913\n",
            "Iteration 107, loss = 0.20974215\n",
            "Iteration 108, loss = 0.20510199\n",
            "Iteration 109, loss = 0.19192099\n",
            "Iteration 110, loss = 0.19723780\n",
            "Iteration 111, loss = 0.20592824\n",
            "Iteration 112, loss = 0.19755548\n",
            "Iteration 113, loss = 0.18433940\n",
            "Iteration 114, loss = 0.18387677\n",
            "Iteration 115, loss = 0.20863548\n",
            "Iteration 116, loss = 0.19340608\n",
            "Iteration 117, loss = 0.18166625\n",
            "Iteration 118, loss = 0.17021939\n",
            "Iteration 119, loss = 0.17064532\n",
            "Iteration 120, loss = 0.16858490\n",
            "Iteration 121, loss = 0.16768191\n",
            "Iteration 122, loss = 0.18804361\n",
            "Iteration 123, loss = 0.18880749\n",
            "Iteration 124, loss = 0.17843283\n",
            "Iteration 125, loss = 0.16419206\n",
            "Iteration 126, loss = 0.15154813\n",
            "Iteration 127, loss = 0.15101656\n",
            "Iteration 128, loss = 0.14802707\n",
            "Iteration 129, loss = 0.14321654\n",
            "Iteration 130, loss = 0.15121664\n",
            "Iteration 131, loss = 0.14614293\n",
            "Iteration 132, loss = 0.14049097\n",
            "Iteration 133, loss = 0.14393414\n",
            "Iteration 134, loss = 0.13172598\n",
            "Iteration 135, loss = 0.12880650\n",
            "Iteration 136, loss = 0.12428388\n",
            "Iteration 137, loss = 0.13287889\n",
            "Iteration 138, loss = 0.17645517\n",
            "Iteration 139, loss = 0.17949897\n",
            "Iteration 140, loss = 0.14981154\n",
            "Iteration 141, loss = 0.16290520\n",
            "Iteration 142, loss = 0.14761345\n",
            "Iteration 143, loss = 0.13037951\n",
            "Iteration 144, loss = 0.12443473\n",
            "Iteration 145, loss = 0.11948369\n",
            "Iteration 146, loss = 0.12227086\n",
            "Iteration 147, loss = 0.10646742\n",
            "Iteration 148, loss = 0.10804418\n",
            "Iteration 149, loss = 0.12221082\n",
            "Iteration 150, loss = 0.12366502\n",
            "Iteration 151, loss = 0.11718816\n",
            "Iteration 152, loss = 0.10237094\n",
            "Iteration 153, loss = 0.11904369\n",
            "Iteration 154, loss = 0.10084209\n",
            "Iteration 155, loss = 0.10322407\n",
            "Iteration 156, loss = 0.09527029\n",
            "Iteration 157, loss = 0.09495027\n",
            "Iteration 158, loss = 0.09056755\n",
            "Iteration 159, loss = 0.10464835\n",
            "Iteration 160, loss = 0.10585488\n",
            "Iteration 161, loss = 0.08642214\n",
            "Iteration 162, loss = 0.08693345\n",
            "Iteration 163, loss = 0.08913583\n",
            "Iteration 164, loss = 0.09337335\n",
            "Iteration 165, loss = 0.08368405\n",
            "Iteration 166, loss = 0.08020344\n",
            "Iteration 167, loss = 0.08284913\n",
            "Iteration 168, loss = 0.09948794\n",
            "Iteration 169, loss = 0.09931214\n",
            "Iteration 170, loss = 0.10137863\n",
            "Iteration 171, loss = 0.09320946\n",
            "Iteration 172, loss = 0.09654494\n",
            "Iteration 173, loss = 0.09418200\n",
            "Iteration 174, loss = 0.08556127\n",
            "Iteration 175, loss = 0.07533629\n",
            "Iteration 176, loss = 0.09497991\n",
            "Iteration 177, loss = 0.08581503\n",
            "Iteration 178, loss = 0.07778394\n",
            "Iteration 179, loss = 0.07709202\n",
            "Iteration 180, loss = 0.08639220\n",
            "Iteration 181, loss = 0.08183354\n",
            "Iteration 182, loss = 0.09687969\n",
            "Iteration 183, loss = 0.08536679\n",
            "Iteration 184, loss = 0.06872352\n",
            "Iteration 185, loss = 0.06249096\n",
            "Iteration 186, loss = 0.06085558\n",
            "Iteration 187, loss = 0.06073799\n",
            "Iteration 188, loss = 0.06058903\n",
            "Iteration 189, loss = 0.05958293\n",
            "Iteration 190, loss = 0.05474810\n",
            "Iteration 191, loss = 0.05303712\n",
            "Iteration 192, loss = 0.05383793\n",
            "Iteration 193, loss = 0.05368312\n",
            "Iteration 194, loss = 0.05259091\n",
            "Iteration 195, loss = 0.05172249\n",
            "Iteration 196, loss = 0.05258528\n",
            "Iteration 197, loss = 0.04963838\n",
            "Iteration 198, loss = 0.05043710\n",
            "Iteration 199, loss = 0.05264936\n",
            "Iteration 200, loss = 0.04775834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=100, learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=20, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=1, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfLqOod0-33z"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGMQsFPF1QyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd65200-3504-498a-df97-6b23157787d3"
      },
      "source": [
        "print('### Training ###')\n",
        "ytrue = np.argmax(ytrain, axis=1)\n",
        "ypred = mlp.predict(Xtrain)\n",
        "ypred = np.argmax(ypred, axis=1)\n",
        "report = classification_report(ytrue, ypred)\n",
        "print(report)\n",
        "\n",
        "mat = confusion_matrix(ytrue,ypred)\n",
        "print(mat)\n",
        "print()\n",
        "print('### Testing ###')\n",
        "ytrue = np.argmax(ytest, axis=1)\n",
        "ypred = mlp.predict(Xtest)\n",
        "ypred = np.argmax(ypred, axis=1)\n",
        "report = classification_report(ytrue, ypred)\n",
        "print(report)\n",
        "mat = confusion_matrix(ytrue,ypred)\n",
        "print(mat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Training ###\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       772\n",
            "           1       1.00      1.00      1.00       751\n",
            "\n",
            "    accuracy                           1.00      1523\n",
            "   macro avg       1.00      1.00      1.00      1523\n",
            "weighted avg       1.00      1.00      1.00      1523\n",
            "\n",
            "[[770   2]\n",
            " [  0 751]]\n",
            "\n",
            "### Testing ###\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       258\n",
            "           1       0.82      0.82      0.82       250\n",
            "\n",
            "    accuracy                           0.82       508\n",
            "   macro avg       0.82      0.82      0.82       508\n",
            "weighted avg       0.82      0.82      0.82       508\n",
            "\n",
            "[[214  44]\n",
            " [ 45 205]]\n"
          ]
        }
      ]
    }
  ]
}